---
layout: post
title: "The Note of ALGO 01"
description: ""
category: "Tech" 
tags: ["Algorithms", "Notes"]
---
{% include JB/setup %}

1-1 Introduction : Why Study Algorithms ?
=========================================

* Important for all other branches of computer science
* Plays a key role in modern technological innovation
* Provides novel "lens" on processes outside of computer science and technology

1-2 Integer Multiplication
==========================

> Perheps the most important principle for the good algorithm designer is to refuse to be content.

There thress important part of algorithms.

* Input
* Output
* Primitive Operation

1-3 Karatsuba Multiplication
============================

1. Compute `a * c`
2. Compute `b * d`
3. Compute `(a + b) * (c + d)`
4. Compute 3 - 2 - 1
5. `1 * 1000 + 2 + 4 * 100 = result`

Recursive Algorithm
-------------------

Write $$x = 10^{\frac{n}{2}}a + b$$ and $$y = 10^{\frac{n}{2}}c + d$$.
Where $$a,b,c,d$$ are $$\frac{n}{2} - digit$$ numbers.
Then $$x*y = (10^{\frac{n}{2}}a+b)(10^{\frac{n}{2}}c+d) = 10^nac+10^{\frac{n}{2}}(ad+bc)+bd^*$$.
Idea: Recurisively compute $$ca,cd,bc,bd$$ then compute * in the straight forward way.
Recall: $$x*y = 10^nac+10^{\frac{n}{2}}(ac+bc)+bd^*$$

1. Recurisively compute $$ac$$
2. Recurisively compute $$bd$$
3. Recurisively compute $$(a+b)(c+d)=ac+ad+bc+bd$$

Gauss's trick: $$3 - 1 - 2 = ad + bc$$
Upshot: Only need 3 recursive multiplications(and some additions)

1-4 About the course
====================

I
-

* Vocabulary
* Divide and conquer(design paradigm)
* Randomization
* Primitives
* Use and implementation

II
--

* Greedy
* Dynamic programming
* NP-complete problems

Skill
-----

* Become a better programmer
* Sharpen your mathematical and analytical skills
* Start "thinking algorithmically"
* Litercy with computer science's "greatest hits"
* Ace your technical interviews

Book
----

* Mathematics for Computer Science
* Algorithm Design
* Algorithm
* Introduction to Algorithms
* Data Structures and Algorithms : The Basic Toolbox

1-5 Merge Sort Motivation and Example
=====================================

Why merge sort
--------------

* Good introduction to divide and conquer
	* Improves *Selection*, *Insertion*, *Bubble*
* Calibrate your preparation
* Motivates guiding principles for algorithm analysis(Worst-case and asymoptotic analysis)
* Analysis generalizes to "Master Method"

The sorting problem
-------------------

*Input* array of n numbers, unsorted

*Output* Same numbers, sorted in increasing order

1-6 Merge Sort : Pseudocode
===========================

Pseudocode for Merge
--------------------

C = output array [length = n]
A = 1st sorted array [n/2]
B = 2rd sorted array [n/2]

~~~
for k=1 to n
	if A(i) < B(i)
		C(k) = A(i)
		i ++
	else [B(j) < A(i)]
		C(k) = B(j)
		j ++
end
~~~

Running Time of Merge
---------------------

m numbers array, $$Running Time \le 4m+2 \le 6m \le 6n\log_{2}n+6n$$

[Running time = number of lines of code executed]

1-7 Merge Sort Analysis
=======================

Running time of Merge Sort : $$6n\log_2n+6n$$

Recursion Tree (Assuming n = power of 2)
* The level number of Tree : $$\log_2n$$
* First level : $$0$$
* Last level : $$\log_2n

At level $$\log_2n$$ : it will be single element arrays

At each level $$j=0,1,2,\dots ,\log_2n$$, there are $$2^j$$ subproblems, each of size $$\frac{n}{2^j}$$.

Total number of operations at level $$j=0,1,\dots,\log_2n$$, $$\le 2^j * 6(\frac{n}{2^j})=6n$$.

In the function:

$$2^j$$
: number of level-j subproblems

$$6$$
: work per level-j subproblems

$$\frac{n}{2^j}$$
: work per level-j subproblems

Total : $$6n(\log_2n+1)$$

$$6n$$
: work per level

$$\log_2n+1$$
: number of levels

1-8 Guiding Principles
======================

Principle #1
------------

* Our running time bound holds for every input of length n
* As opposed to
	* Average-case anlysis
	* Benchmarks(requires domain knowledge)

BOUNS
: Worst case usually become easier to analysis

Principle #2
------------

Won't pay much attention to constant factors, lower-order terms.

Sweet spot
: Mathematical tsactability and predictive power.

Justifications
: 1. way easier
  2. constants depend on architecture/compiler/programmer anyways
  3. lose very little predictive power

Principles #3
-------------

Asymptotic analysis
: focus on running time for large input sizes n.

Justifications
: Only big problems are interesting.

What Is a "Fast" Algorithm ?
----------------------------

This course
: Adopt these three biases as guiding principles.

Fast algorithm
: worst-case running time grows slowly with input size.

Usually
: want as close to linear ($$O(n)$$) as possible.
